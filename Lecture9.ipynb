{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdff6eb6-0ffe-4e59-a7bc-f153f2788657",
   "metadata": {},
   "source": [
    "# BIOS470/570 Lecture 9\n",
    "\n",
    "## Last time we covered:\n",
    "* ### Clustering data \n",
    "* ### seaborn plotting package\n",
    "* ### Gene ontology with gget\n",
    "\n",
    "## Today we will cover:\n",
    "* ### Dimensionality reduction with PCA. \n",
    "\n",
    "#### We will use the pca command from the scikit-learn library which has many functions for machine learning. \n",
    "#### Install command: conda install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ef5de-3a1a-4722-bd3f-9d7c396d0556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import decomposition, preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67162f7f-508c-4277-b6e2-43a6886259a9",
   "metadata": {},
   "source": [
    "### Data that goes into PCA is often scaled to avoid the variance being dominated by the components with the largest numbers. Let's look at a simple example of this scaling. First lets make a dataframe where the scale of x and y is very different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92466738-fc9d-46c8-9c99-6f83c1f1a1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a dataframe with random data in columns labeled x and y:\n",
    "rand_data = pd.DataFrame({'x':np.random.random(100),'y':100*np.random.random(100)})\n",
    "# use seaborn to scatterplot:\n",
    "sns.scatterplot(rand_data, x = 'x', y = 'y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a79b7d-aeec-45f2-a930-fafaf4cf94f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The following applies a standard scaling from the scikit-learn library. The standard scaling is for each variable to subtract its mean and divide by its standard deviation: i.e. $x_{scaled} = (x - \\langle x \\rangle )/ \\rm{std}(x)$\n",
    "\n",
    "### This example also shows how many of the functions of the scikit-learn work. You define the model and then fit it, which actually runs the algorithm. The fit modifies the model object in place. Then, running the transform command actually returns the transformed data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f174b-ee60-4da0-a47f-8e2c78997686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler() #define the model\n",
    "scaler.fit(rand_data) #apply it to the data\n",
    "\n",
    "#Now get the transformed data. \n",
    "# By default sklearn outputs this as a numpy array, but we can put it back into a dataframe, with the same column names:\n",
    "new_data = pd.DataFrame(scaler.transform(rand_data),columns=rand_data.columns)\n",
    "sns.scatterplot(new_data,x = 'x',y = 'y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d014281",
   "metadata": {},
   "source": [
    "### Now let's look at a dataset with a correlation between x and y. We will define a dataframe with x and y, where the value of y is just x with some added random number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b77e3c5-8f61-4159-9eb6-15505286b648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define some random numbers\n",
    "x = np.random.random(100)\n",
    "# a data frame where x are the random numbers and y is x with added noise. \n",
    "rand_data = pd.DataFrame({'x':x,'y':x+0.5*np.random.random(100)})\n",
    "#make a scatterplot of the dataframe\n",
    "sns.scatterplot(rand_data, x = 'x', y = 'y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d59906e",
   "metadata": {},
   "source": [
    "### Let's apply the same standard scaler to the data, it will look basically the same but the means will have shifted, and the scale changed.  Note that some information on the relative magnitudes or dispersions between components has been lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1ff79b-1f09-492c-b194-98830b215fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler() #make the scalar object\n",
    "scaler.fit(rand_data) #apply it to the data\n",
    "new_data = pd.DataFrame(scaler.transform(rand_data), columns=rand_data.columns) #get the transformed data\n",
    "sns.scatterplot(new_data, x = 'x',y = 'y'); #plot it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ace8fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Now let's apply PCA to this dataset. First, we make the pca model object with decomposition.pca, then we run fit to actual do the PCA. This will add some variables into the model. First, let's look at explained variance ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530318ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2) #specify the models with its parameters, for example, the number of components to return\n",
    "pca.fit(new_data) #actually run the pca\n",
    "pca.explained_variance_ratio_ #how much variance does each principle component contain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175980c7",
   "metadata": {},
   "source": [
    "### There are two components to the PCA, because that is how many we specified. That is also the maximum number because the originail data was 2 dimensional.  ~95% of the variance in the data can be explained by the first component. \n",
    "\n",
    "### Now let's get the transformed data. We will put it in a new dataframe with the columns labeled pc1 and pc2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f024ed-aa80-4856-8a84-56646402d7c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformed_data = pd.DataFrame(pca.transform(new_data), columns=['PC1','PC2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5fa23e-7719-403f-a460-6aa63a548fff",
   "metadata": {},
   "source": [
    "### Now lets plot some of the outputs of this. We can look at two different things: the principle components in the coodinates of the original data (given by pca.components_ and the transformed data which is given by pca.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd0f98-6eef-4ef2-885b-f945fdd250c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#axes[0].scatter(new_data[:,0],new_data[:,1])\n",
    "fig = plt.figure(figsize = (12,5))\n",
    "axes = fig.subplots(1,2)\n",
    "sns.scatterplot(new_data, x = 'x',y='y',ax = axes[0]) #plot the original data\n",
    "#add vectors for the principle components\n",
    "axes[0].arrow(0,0, pca.components_[0,0], pca.components_[0,1], head_width = 0.1, color = 'r', label = 'pc1')\n",
    "axes[0].arrow(0,0, pca.components_[1,0], pca.components_[1,1], head_width = 0.1, color = 'r')\n",
    "#this sets the x and y scales to be equal so if the vectors are at right angles, we will see it that way. \n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].set_title('Original data with principle components indicated')\n",
    "\n",
    "#Now plot the transformed data. The coordinates are no longer the original x and y but the values of the principle components for each data point\n",
    "sns.scatterplot(transformed_data, x = 'PC1', y = 'PC2', ax = axes[1])\n",
    "axes[1].set_title('Transformed data')\n",
    "axes[1].set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8bf0e9-e4ac-4775-b842-b1009ff632f3",
   "metadata": {},
   "source": [
    "### Now let's see how this looks on a real dataset. We will look at the RNAseq from frog development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dc61e2-eebf-466c-96f3-8ebcbbce8cff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/xen_uic_hik_stage8_13_30min.tsv',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750607a6-a4f1-4ad5-852b-fe5a615a7b7b",
   "metadata": {},
   "source": [
    "### Apply the scaler to the data as well did before. If you want to skip this step, you can do it with the PCA together by passing the argument whiten=True when you define the model e.g. PCA(n_components = 4, whiten = True). \n",
    "\n",
    "### We also need to transpose the data - we want to consider each sample as a datapoint and replace the large space of genes for this sample by the smaller sample of principle components. To do so, we need to have each row be one sample and each column be a gene:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb3a657-c97f-4c21-a77d-b47ae9bba029",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_numeric = data.iloc[:,1:].transpose().to_numpy() #get only the numeric data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(data_numeric)\n",
    "data_scaled = scaler.transform(data_numeric) #get the scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf8991-e9dc-45ea-b3a9-5e0c6139ba51",
   "metadata": {},
   "source": [
    "### Here the fit_transform both does the fitting and returns the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed49f911-ff2a-4ad7-94b7-c3f8520791b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=10)\n",
    "transformed_data = pca.fit_transform(data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baad3626-a262-41be-badc-14d09623bad0",
   "metadata": {},
   "source": [
    "### let's put this in a dataframe with column labels for the principle components, and include a column which labels the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b3de9-70ea-47de-8adc-9e4794756055",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_labels = []\n",
    "for ii in range(1,11):\n",
    "    pc_labels.append('PC' + str(ii))\n",
    "    \n",
    "transformed_data = pd.DataFrame(transformed_data,columns=pc_labels)\n",
    "transformed_data[\"sample_labels\"] = data.columns[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b74f4a-5d7d-44e7-9572-1d1a91fe37d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Look at the fraction of variance which is explained by each principle component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d6eb77-8147-4cf3-acac-12f90f43b075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dadfd60-4dd3-486b-b1ae-f59769ad8109",
   "metadata": {},
   "source": [
    "### We will use this function, borrowed from [here](https://stackoverflow.com/questions/46027653/adding-labels-in-x-y-scatter-plot-with-seaborn) to make plots of principle components with the samples labeled. It just uses the seaborn scatterplot together with the .text method to add text to each point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97b159-9db4-4f8e-83ee-06e33303c7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scatter_text(x, y, text_column, data):\n",
    "    \"\"\"Scatter plot with country codes on the x y coordinates\n",
    "       Based on this answer: https://stackoverflow.com/a/54789170/2641825\"\"\"\n",
    "    # Create the scatter plot\n",
    "    p1 = sns.scatterplot(x = x,y =  y, data=data, s = 100, legend=False)\n",
    "    # Add text besides each point\n",
    "    for line in range(0,data.shape[0]):\n",
    "         p1.text(data[x][line]+0.01, data[y][line], \n",
    "                 data[text_column][line], horizontalalignment='left', size=18, color='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d04c71-31b6-4760-942d-1cb93ab52044",
   "metadata": {},
   "source": [
    "### Make plots of PC1 vs PC2, PC1 vs PC3, and PC2 vs PC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b79635-03b8-4297-aa60-81ce0d3c8c23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "scatter_text('PC1','PC2',\"sample_labels\",transformed_data)\n",
    "ax1.set_xlabel('PC1',fontsize=24)\n",
    "ax1.set_ylabel('PC2',fontsize=24)\n",
    "\n",
    "ax1 = fig.add_subplot(1,2,2)\n",
    "scatter_text('PC1','PC3',\"sample_labels\",transformed_data)\n",
    "ax1.set_xlabel('PC1',fontsize=24)\n",
    "ax1.set_ylabel('PC3',fontsize=24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51908f5-bad5-431a-84de-1caf133de57c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The samples labeled UIC_... are untreated samples at different times while hiK_... are treated samples. Note how they take different routes the princple component space before joining again.\n",
    "\n",
    "### Now lets look at PC2 vs PC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f84750e-f373-4dd8-90a5-17948a8f44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax1 = fig.add_subplot()\n",
    "scatter_text('PC2','PC3',\"sample_labels\",transformed_data)\n",
    "ax1.set_xlabel('PC2',fontsize=24)\n",
    "ax1.set_ylabel('PC3',fontsize=24);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc108e2e-6139-4add-8f00-8ce3ea942763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
